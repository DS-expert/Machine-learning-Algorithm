{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_diabetes(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
       "          0.01990749, -0.01764613],\n",
       "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
       "         -0.06833155, -0.09220405],\n",
       "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
       "          0.00286131, -0.02593034],\n",
       "        ...,\n",
       "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
       "         -0.04688253,  0.01549073],\n",
       "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
       "          0.04452873, -0.02593034],\n",
       "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
       "         -0.00422151,  0.00306441]]),\n",
       " array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "        220.,  57.]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_sk = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  37.90402135, -241.96436231,  542.42875852,  347.70384391,\n",
       "        -931.48884588,  518.06227698,  163.41998299,  275.31790158,\n",
       "         736.1988589 ,   48.67065743]),\n",
       " np.float64(151.34560453985995))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_, lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4526027629719195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score_sk = r2_score(y_test, y_pred_sk)\n",
    "\n",
    "r2_score_sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Gradient_Descent():\n",
    "\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        X_train = np.insert(X_train, 0,1,axis=1)\n",
    "        self.coef_ = np.random.randn(X_train.shape[1]) * 0.01\n",
    "        self.intercept_ = 0\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            y_hat = np.dot(X_train, self.coef_ ) + self.intercept_\n",
    "            intercept_slope  = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ + (self.lr * intercept_slope)\n",
    "\n",
    "            coef_slope = -2 * np.mean((y_train - y_hat)[:, np.newaxis] * X_train, axis=0)\n",
    "            self.coef_ = self.coef_ + (self.lr * coef_slope)\n",
    "\n",
    "        print(f\"Coef_: {self.coef_}, intercept_: {self.intercept_}\")\n",
    "        print(f\"Coef Shape: {self.coef_.shape}\")\n",
    "        print(f\"X_train Shape: {X_train.shape}\")\n",
    "        print(f\"y_hat: {y_hat}\")\n",
    "        print(f\"Interecept_slope: {intercept_slope}, Coef Slope: {coef_slope}\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.insert(X_test, 0,1, axis=1)\n",
    "        y_pred = self.intercept_ + np.dot(X_test, self.coef_)\n",
    "        return y_pred\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgd = Batch_Gradient_Descent(learning_rate=0.000001, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coef_: [-0.15294936 -0.0097822   0.00423783 -0.00937663 -0.0110347  -0.0048782\n",
      " -0.00442949  0.00567004 -0.00254194  0.0187287  -0.00800669], intercept_: -0.15388916175909145\n",
      "Coef Shape: (11,)\n",
      "X_train Shape: (353, 11)\n",
      "y_hat: [-0.30761093 -0.30608034 -0.30746444 -0.3045759  -0.30406519 -0.3045303\n",
      " -0.30698398 -0.30639398 -0.30670304 -0.30482089 -0.30610044 -0.30684889\n",
      " -0.30554227 -0.30807562 -0.30379114 -0.30688446 -0.30379394 -0.30617516\n",
      " -0.30756157 -0.30605382 -0.30771839 -0.30607047 -0.30603542 -0.30738563\n",
      " -0.30731923 -0.30541414 -0.30498896 -0.30562058 -0.30586347 -0.30476511\n",
      " -0.30528227 -0.30629247 -0.3054206  -0.30765677 -0.30627223 -0.30732951\n",
      " -0.30548813 -0.30639389 -0.3051412  -0.3069399  -0.30389578 -0.30888099\n",
      " -0.30561399 -0.30635852 -0.30609404 -0.30924428 -0.30630889 -0.30681709\n",
      " -0.30654759 -0.30459243 -0.30861447 -0.30439904 -0.30702001 -0.30746956\n",
      " -0.30490887 -0.30748655 -0.30705995 -0.30641063 -0.30604021 -0.30587442\n",
      " -0.30826473 -0.30737033 -0.30732332 -0.30692439 -0.30765806 -0.30717308\n",
      " -0.30608118 -0.3050063  -0.30633147 -0.30824744 -0.30716866 -0.30568759\n",
      " -0.30505441 -0.30661869 -0.30523742 -0.30479054 -0.30579017 -0.30743302\n",
      " -0.30571041 -0.30653084 -0.30547878 -0.30625863 -0.30331218 -0.30656137\n",
      " -0.30569982 -0.30650684 -0.30607267 -0.30900063 -0.3067312  -0.30721276\n",
      " -0.30630513 -0.30610684 -0.30824106 -0.3048181  -0.30540597 -0.30558804\n",
      " -0.30481521 -0.30514984 -0.30780881 -0.30651944 -0.30604169 -0.30465799\n",
      " -0.30648531 -0.30787516 -0.30608503 -0.30765126 -0.30487634 -0.30798669\n",
      " -0.3073228  -0.30678663 -0.30658746 -0.3068773  -0.30942214 -0.30761779\n",
      " -0.30519777 -0.30536971 -0.30712112 -0.30864075 -0.30329961 -0.30675528\n",
      " -0.30673769 -0.30666558 -0.30297535 -0.30653309 -0.30481838 -0.30911505\n",
      " -0.30773518 -0.30657144 -0.30609387 -0.30588084 -0.30776263 -0.30519316\n",
      " -0.30783011 -0.30494091 -0.30570615 -0.30636402 -0.30484882 -0.3073449\n",
      " -0.30825658 -0.30820876 -0.30638663 -0.30787373 -0.30641862 -0.30682613\n",
      " -0.30805241 -0.3078544  -0.30809472 -0.3059997  -0.30769674 -0.30784039\n",
      " -0.30415253 -0.30658867 -0.30530852 -0.30559621 -0.30372226 -0.30516983\n",
      " -0.30560235 -0.30493047 -0.30387396 -0.30727028 -0.30859263 -0.30537763\n",
      " -0.30485585 -0.30479708 -0.30700429 -0.30552765 -0.30629105 -0.30565081\n",
      " -0.30823882 -0.30607032 -0.30616937 -0.30724572 -0.30635188 -0.30786327\n",
      " -0.30302746 -0.30742936 -0.30639324 -0.30659023 -0.30588139 -0.30411172\n",
      " -0.30774631 -0.30808797 -0.30662832 -0.30518421 -0.30650194 -0.30434627\n",
      " -0.30540844 -0.3059915  -0.3050368  -0.30655142 -0.30811704 -0.30678137\n",
      " -0.3049521  -0.30652759 -0.30698396 -0.30347993 -0.3059043  -0.30584574\n",
      " -0.30749621 -0.30468994 -0.30531348 -0.30696278 -0.30601647 -0.3047412\n",
      " -0.30505107 -0.30673597 -0.30652954 -0.30346645 -0.30716362 -0.30509264\n",
      " -0.3041029  -0.30675929 -0.30772278 -0.30583433 -0.30708092 -0.30680654\n",
      " -0.30358399 -0.30790561 -0.30651485 -0.30828864 -0.30672852 -0.30597515\n",
      " -0.30662597 -0.30661079 -0.30540302 -0.30608928 -0.30407512 -0.30661834\n",
      " -0.30572868 -0.30464981 -0.30513057 -0.30563893 -0.30832716 -0.3048164\n",
      " -0.30330028 -0.30858858 -0.30662093 -0.30731697 -0.30555515 -0.30634895\n",
      " -0.30565688 -0.30690299 -0.30631735 -0.30819935 -0.30577094 -0.30736217\n",
      " -0.30810822 -0.30601116 -0.30606836 -0.30470393 -0.30703776 -0.3059519\n",
      " -0.30577377 -0.30635287 -0.30599276 -0.30578509 -0.30756227 -0.3070659\n",
      " -0.30456448 -0.3078963  -0.30586033 -0.30534744 -0.30706934 -0.30716312\n",
      " -0.30641657 -0.30754063 -0.30488239 -0.30621253 -0.30723058 -0.30623926\n",
      " -0.30753268 -0.30718142 -0.30726079 -0.30489016 -0.30578955 -0.30793455\n",
      " -0.30617703 -0.30857106 -0.30646186 -0.30655983 -0.30769942 -0.30567754\n",
      " -0.30802114 -0.30551444 -0.30740827 -0.305457   -0.30745066 -0.30501406\n",
      " -0.30527431 -0.30474643 -0.30649865 -0.30694741 -0.30578306 -0.30621065\n",
      " -0.30702354 -0.30719829 -0.3074184  -0.30522369 -0.30573865 -0.30493336\n",
      " -0.30647518 -0.30624209 -0.30434982 -0.30598835 -0.30758277 -0.30545884\n",
      " -0.30659533 -0.30590886 -0.30631892 -0.30616469 -0.30686616 -0.30511858\n",
      " -0.30683915 -0.30475345 -0.30588099 -0.30659458 -0.30453649 -0.30500677\n",
      " -0.30479603 -0.30704413 -0.30513171 -0.30759795 -0.30697005 -0.3056988\n",
      " -0.30702332 -0.30491516 -0.3068743  -0.30417102 -0.30627584 -0.30802673\n",
      " -0.30613422 -0.30827544 -0.30465129 -0.30498959 -0.30388757 -0.30759772\n",
      " -0.30584398 -0.30733783 -0.30572206 -0.3053398  -0.30554605 -0.30811803\n",
      " -0.30508485 -0.30758191 -0.30393421 -0.3047462  -0.30734773 -0.30836511\n",
      " -0.30419386 -0.30665255 -0.30596197 -0.30607033 -0.30555452]\n",
      "Interecept_slope: -308.0856029829787, Coef Slope: [-3.08085603e+02 -1.86175763e+00 -1.09447779e-01 -4.98570164e+00\n",
      " -3.71952399e+00 -1.31340725e+00 -9.00534771e-01  3.11707590e+00\n",
      " -3.27876711e+00 -4.48620720e+00 -3.52284365e+00]\n"
     ]
    }
   ],
   "source": [
    "bgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_custom(y_true, y_pred):\n",
    "\n",
    "    mean_value = np.mean(y_true)\n",
    "\n",
    "    SSE = np.sum((y_true - y_pred) ** 2)\n",
    "    TSS = np.sum((y_true - mean_value) ** 2)\n",
    "\n",
    "    r2_score = 1 - (SSE / TSS)\n",
    "    return r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-4.027818624331719)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score_c = r2_score_custom(y_test, y_pred)\n",
    "\n",
    "r2_score_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.027818624331719"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
